Download Link: https://assignmentchef.com/product/solved-data201-mid-term-test
<br>
(Total 50 Marks)

<ol>

 <li>Go to the Moral Machine website (http://moralmachine.mit.edu/) and do the “Judge” exercise. Also watch this clip https://www.youtube.com/watch?v=nhCh1pBsS80 about the Moral Machine, driverless cars, and related issues. Then write 200-300 words on how you would approach working on a team of engineers and programmers designing a driverless car with the capability to make choices of the kind that are needed in the Judging exercise. In your answer consider the ethical choices that are implied – including a discussion of which ones the team <strong>does </strong>have responsibility for, and which ones they <strong>don’t</strong>. <strong>(10 Marks) </strong>Answer:</li>

</ol>

ANS: Answers should comment on:

<ul>

 <li>The difficulty of making choices which may be culturally relative, and differ strongly between individuals</li>

 <li>The difference between action and inaction, which may lead to a difference in moral responsibility</li>

 <li>The difficulty in working with a team which may have a variety of views on values</li>

 <li>The counterbalancing value of having driverless cars making better and faster decisions that humans in scenarios where there is <strong>not </strong>a choice between two bad outcomes, but instead where lives can be saved.</li>

 <li>How decisions will be made: will the team follow the decision of its manager, the consensus of its members, or will it seek to follow societal norms, even if the team members disagree?</li>

 <li>The role of regulators</li>

 <li>Could comment on the fact that even though the algorithm may make mistakes/have unfortunate outcomes, overall the introduction of driverless cars will lead to fewer deaths</li>

 <li>Manufacturers should be open and honest about their design decisions</li>

 <li>A phase of public education may be required when driverless cars are introduced, to explain these risks</li>

 <li>Is it feasible for the car to identify and classify its victims in the time available?</li>

 <li>Will the safety algorithm degrade the performance of the other algorithms the car is running?</li>

 <li>Should the car’s owner be able to set the level of selfishness of the car’s actions? what would society think of that?</li>

 <li>What will the impact on the car’s market be if people think the car will decide to kill them in certain circumstances?</li>

</ul>

(Answers cannot deny the existence of the problem: and say that if the car obeys the road rules then these choices will never arise, or that these events are rare. Accidents happen all the time, and designers always need to consider low-probability-high-consequence events.)

<ol start="2">

 <li>The following table of relationship status by age group potentially exposes characteristics of the individuals it contains, with one individual at particular risk of disclosure. Confidentialise the table in two ways (cell suppression to conceal the riskiest cell, and random rounding to base 3) – clearly explain the steps you have taken in each case, and comment on the advantages/disadvantages of the two approaches. <strong>(10 Marks) </strong>Relationship Status by age:</li>

</ol>

<table width="496">

 <tbody>

  <tr>

   <td width="88">Age Group</td>

   <td width="56">Single</td>

   <td width="146">Opposite sex couple</td>

   <td width="122">Same sex couple</td>

   <td width="53">Other</td>

   <td width="32">Total</td>

  </tr>

  <tr>

   <td width="88">15-19</td>

   <td width="56">0</td>

   <td width="146">0</td>

   <td width="122">1</td>

   <td width="53">0</td>

   <td width="32">1</td>

  </tr>

  <tr>

   <td width="88">20-24</td>

   <td width="56">8</td>

   <td width="146">7</td>

   <td width="122">4</td>

   <td width="53">2</td>

   <td width="32">21</td>

  </tr>

  <tr>

   <td width="88">25-29</td>

   <td width="56">14</td>

   <td width="146">10</td>

   <td width="122">4</td>

   <td width="53">0</td>

   <td width="32">28</td>

  </tr>

  <tr>

   <td width="88">30-34</td>

   <td width="56">7</td>

   <td width="146">8</td>

   <td width="122">0</td>

   <td width="53">1</td>

   <td width="32">16</td>

  </tr>

  <tr>

   <td width="88">34-39</td>

   <td width="56">3</td>

   <td width="146">2</td>

   <td width="122">3</td>

   <td width="53">1</td>

   <td width="32">9</td>

  </tr>

  <tr>

   <td width="88">Total</td>

   <td width="56">32</td>

   <td width="146">27</td>

   <td width="122">12</td>

   <td width="53">4</td>

   <td width="32">75</td>

  </tr>

 </tbody>

</table>

Answer:

<strong>Method 1: </strong>Cell suppression – suppress the risky cell (15-19, status “Same sex couple”), and three others so that the risky cell cannot be deduced. Firstly we suppress the cell itself, then choose one column in the same row (“Opposite sex couple”), one row in the same column (30-34), and then consequentially suppress a fourth cell (30-34, status “Same sex couple”).

<table width="496">

 <tbody>

  <tr>

   <td width="88">Age Group</td>

   <td width="56">Single</td>

   <td width="146">Opposite sex couple</td>

   <td width="122">Same sex couple</td>

   <td width="53">Other</td>

   <td width="32">Total</td>

  </tr>

  <tr>

   <td width="88">15-19</td>

   <td width="56">0</td>

   <td width="146">–</td>

   <td width="122">–</td>

   <td width="53">0</td>

   <td width="32">1</td>

  </tr>

  <tr>

   <td width="88">20-24</td>

   <td width="56">8</td>

   <td width="146">7</td>

   <td width="122">4</td>

   <td width="53">2</td>

   <td width="32">21</td>

  </tr>

  <tr>

   <td width="88">25-29</td>

   <td width="56">14</td>

   <td width="146">10</td>

   <td width="122">4</td>

   <td width="53">0</td>

   <td width="32">28</td>

  </tr>

  <tr>

   <td width="88">30-34</td>

   <td width="56">7</td>

   <td width="146">–</td>

   <td width="122">–</td>

   <td width="53">1</td>

   <td width="32">16</td>

  </tr>

  <tr>

   <td width="88">34-39</td>

   <td width="56">3</td>

   <td width="146">2</td>

   <td width="122">3</td>

   <td width="53">1</td>

   <td width="32">9</td>

  </tr>

  <tr>

   <td width="88">Total</td>

   <td width="56">32</td>

   <td width="146">27</td>

   <td width="122">12</td>

   <td width="53">4</td>

   <td width="32">75</td>

  </tr>

 </tbody>

</table>

<strong>Method 2: </strong>Random rounding – random round each cell to base 3. If a cell is a multiple of 3 leave it unchanged, otherwise with probability 2/3 round to the nearest multiple of 3, and with probability 1/3 round to the multiple of 3 that is two units away.

<table width="496">

 <tbody>

  <tr>

   <td width="88">Age Group</td>

   <td width="56">Single</td>

   <td width="146">Opposite sex couple</td>

   <td width="122">Same sex couple</td>

   <td width="53">Other</td>

   <td width="32">Total</td>

  </tr>

  <tr>

   <td width="88">15-19</td>

   <td width="56">0</td>

   <td width="146">0</td>

   <td width="122">3</td>

   <td width="53">0</td>

   <td width="32">3</td>

  </tr>

  <tr>

   <td width="88">20-24</td>

   <td width="56">6</td>

   <td width="146">6</td>

   <td width="122">3</td>

   <td width="53">3</td>

   <td width="32">21</td>

  </tr>

  <tr>

   <td width="88">25-29</td>

   <td width="56">15</td>

   <td width="146">12</td>

   <td width="122">3</td>

   <td width="53">0</td>

   <td width="32">27</td>

  </tr>

  <tr>

   <td width="88">30-34</td>

   <td width="56">9</td>

   <td width="146">9</td>

   <td width="122">0</td>

   <td width="53">0</td>

   <td width="32">15</td>

  </tr>

  <tr>

   <td width="88">Age Group</td>

   <td width="56">Single</td>

   <td width="146">Opposite sex couple</td>

   <td width="122">Same sex couple</td>

   <td width="53">Other</td>

   <td width="32">Total</td>

  </tr>

  <tr>

   <td width="88">34-39</td>

   <td width="56">3</td>

   <td width="146">3</td>

   <td width="122">3</td>

   <td width="53">0</td>

   <td width="32">9</td>

  </tr>

  <tr>

   <td width="88">Total</td>

   <td width="56">33</td>

   <td width="146">27</td>

   <td width="122">12</td>

   <td width="53">6</td>

   <td width="32">75</td>

  </tr>

 </tbody>

</table>

(Answers will differ for each student – the above is just one example)

<strong>Comparison</strong>

<ul>

 <li>Both methods remove information from the user.</li>

 <li>With cell suppression we retain exact marginal counts, but can’t use the table for calculations without making a guess at the content of the suppressed cells.</li>

 <li>The consequential cell suppression of four cells in order to conceal the content of just one cell means a lot of safe data are having to be suppressed.</li>

 <li>The counts in the table are still low, and there is still a lot of potential risk in the table: for example we can still deduce that the single 15-19 year old is not single, but is in a couple of <strong>some </strong>sort – Random rounding gives us a value in every cell – However the cells do not add up to the marginal totals, which means the table will be awkward in computations.</li>

</ul>

<ol start="3">

 <li>Study the Python code below and its output the answer the questions that follow. <strong>(10 marks) </strong>Code (the dataset is not provided so you should not try running the code):</li>

</ol>

[ ]: import pandas as pd import numpy as np

from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer

from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, GridSearchCV np.random.seed(0) data = pd.read_csv(“dataset.csv”)

print(data.info()) print(data.survived.unique())

numeric_features = [‘age’, ‘fare’] numeric_transformer = Pipeline(steps=[

(‘imputer’, SimpleImputer(strategy=’median’)), (‘scaler’, StandardScaler())])

categorical_features = [’embarked’, ‘sex’, ‘pclass’] categorical_transformer = Pipeline(steps=[

(‘imputer’, SimpleImputer(strategy=’constant’, fill_value=’missing’)), (‘onehot’, OneHotEncoder(handle_unknown=’ignore’))]) preprocessor = ColumnTransformer(

transformers=[

(‘num’, numeric_transformer, numeric_features),

(‘cat’, categorical_transformer, categorical_features)])

clf = Pipeline(steps=[(‘preprocessor’, preprocessor),

(‘classifier’, LogisticRegression(solver=’lbfgs’))])

X = data.drop(‘survived’, axis=1) y = data[‘survived’]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) print(X_train.shape[0])

clf.fit(X_train, y_train) print(“%.3f” % clf.score(X_test, y_test))

param_grid = {

‘preprocessor__num__imputer__strategy’: [‘mean’, ‘median’],

‘classifier__C’: [0.1, 1.0, 10, 100],

}

grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False) grid_search.fit(X_train, y_train) print(grid_search.best_params_) print(“%.3f” % grid_search.score(X_test, y_test))

Output:

&lt;class ‘pandas.core.frame.DataFrame’&gt; RangeIndex: 1309 entries, 0 to 1308 Data columns (total 14 columns):

<table width="256">

 <tbody>

  <tr>

   <td width="98">pclass</td>

   <td width="158">1309 non-null int64</td>

  </tr>

  <tr>

   <td width="98">survived</td>

   <td width="158">1309 non-null int64</td>

  </tr>

  <tr>

   <td width="98">name</td>

   <td width="158">1309 non-null object</td>

  </tr>

  <tr>

   <td width="98">sex</td>

   <td width="158">1309 non-null object</td>

  </tr>

  <tr>

   <td width="98">age</td>

   <td width="158">1046 non-null float64</td>

  </tr>

  <tr>

   <td width="98">sibsp</td>

   <td width="158">1309 non-null int64</td>

  </tr>

  <tr>

   <td width="98">parch</td>

   <td width="158">1309 non-null int64</td>

  </tr>

  <tr>

   <td width="98">ticket</td>

   <td width="158">1309 non-null object</td>

  </tr>

  <tr>

   <td width="98">fare</td>

   <td width="158">1308 non-null float64</td>

  </tr>

  <tr>

   <td width="98">cabin</td>

   <td width="158">295 non-null object</td>

  </tr>

  <tr>

   <td width="98">embarked</td>

   <td width="158">1307 non-null object</td>

  </tr>

  <tr>

   <td width="98">boat</td>

   <td width="158">486 non-null object</td>

  </tr>

  <tr>

   <td width="98">body</td>

   <td width="158">121 non-null float64</td>

  </tr>

  <tr>

   <td width="98">home.dest</td>

   <td width="158">745 non-null object</td>

  </tr>

 </tbody>

</table>

dtypes: float64(3), int64(4), object(7) memory usage: 143.2+ KB

None

[1 0]

1047

0.790

{‘classifier__C’: 0.1, ‘preprocessor__num__imputer__strategy’: ‘mean’}

0.798

Questions:

<ol>

 <li>How many examples and features are there in the given dataset? (1 mark)</li>

 <li>Which feature has the largest number of missing values? How many? (1 mark)</li>

 <li>Which feature is used as the label for the classification problem in the code? How many classes are there? (1 mark)</li>

 <li>How many samples are there in the test set? (1 mark)</li>

 <li>What are the features used for model training? (1 mark)</li>

 <li>What is the name of the classification algorithm used in the code? (1 mark)</li>

 <li>What was done to the numerical features before model training? (2 marks)</li>

 <li>From the outcome of using GridSearchSV, what should be done to improve the model clf?</li>

</ol>

(1 mark)

<ol>

 <li>If the hyperparameters found from using GridSearchSV are used, will the accuracy of the model on the test set improve? (1 mark) Answer:</li>

 <li>1309 examples and 14 features</li>

 <li>Feature body having 1309-121=1188 missing values</li>

 <li>Feature survived. Two classes: 1 and 0</li>

 <li>The test set has 1309-1047=262 samples.</li>

 <li>5 features: age, fare, embarked, sex, pclass.</li>

 <li>Logistic Regression</li>

 <li>(2 marks) Replace missing values using the median along each of the two columns age and fare, and then standardize those two features by removing the mean and scaling to unit variance.</li>

 <li>Replace missing values using the <em>mean </em>instead of the <em>median </em>along each of the two columns age and fare, and increase regularization with C = 0.1.</li>

 <li>Yes, it will increase from 79.0% to 79.8%.</li>

 <li>The purpose of setting the random_state parameter in train_test_split is: (select all that apply) <strong>(1 mark)</strong>

  <ol>

   <li>To avoid predictable splitting of the data</li>

   <li>To make experiments easily reproducible by always using the same partitioning of the data</li>

   <li>To avoid bias in data splitting</li>

   <li>To split the data into similar subsets so that bias is not introduced into the final results Answer:</li>

  </ol></li>

</ol>

b

<ol start="5">

 <li>Given a dataset with 10,000 observations and 50 features plus one label, what would be the dimensions of X_train, y_train, X_test, and y_test? Assume a train/test split of 75%/25%. <strong>(1 mark)</strong>

  <ol>

   <li>X_train: (2500, ) ; y_train: (2500, 50) ; X_test: (7500, ) ; y_test: (7500, 50)</li>

   <li>X_train: (10000, 28) ; y_train: (10000, ) ; X_test: (10000, 12) ; y_test: (10000, )</li>

  </ol></li>

</ol>

<ul>

 <li>X_train: (2500, 50) ; y_train: (2500, ) ; X_test: (7500, 50) ; y_test: (7500, )</li>

</ul>

<ol>

 <li>X_train: (7500, 50) ; y_train: (7500, ) ; X_test: (2500, 50) ; y_test: (2500, )</li>

 <li>None of the above Answer:</li>

</ol>

d

<ol start="6">

 <li>Which of the following is an example of multiclass classification (select all that apply)? <strong>(1 mark)</strong>

  <ol>

   <li>Classify a set of fruits as apples, oranges, bananas, or lemons</li>

   <li>Predict whether an article is relevant to one or more topics (e.g. sports, politics, finance, science)</li>

   <li>Predicting both the rating and profit of soon to be released movie</li>

   <li>Classify a voice recording as an authorized user or not an authorized user.</li>

  </ol></li>

</ol>

Answer:

a

<ol start="7">

 <li>Looking at the plot below which shows accuracy scores for different values of a regularization parameter <em>lambda</em>, what value of <em>lambda </em>is the best choice for generalization? <strong>(2 marks)</strong></li>

</ol>

Answer:

10

<ol start="8">

 <li>Which of the following is true of cross-validation (select all that apply)? <strong>(1 mark)</strong>

  <ol>

   <li>Helps prevent knowledge about the test set from leaking into the model</li>

   <li>Fits multiple models on different splits of the data</li>

  </ol></li>

</ol>

<ul>

 <li>Increases generalization ability and computational complexity</li>

</ul>

<ol>

 <li>Increases generalization ability and reduces computational complexity</li>

 <li>Removes need for training and test sets Answer: a, b, c</li>

</ol>

<ol start="9">

 <li>A supervised learning model has been built to predict whether someone is infected with a new strain of a virus. The probability of any one person having the virus is 1%. Using accuracy as a metric, what would be a good choice for a baseline accuracy score that the new model would want to outperform? <strong>(1 mark) </strong>Answer:</li>

</ol>

99%

<ol start="10">

 <li>Given the following confusion matrix:</li>

</ol>

Predicted Positive        Predicted Negative

Condition Positive          96                                       4

Condition Negative       8                                          19

Compute the <em>accuracy</em>, <em>precision</em>, <em>recall</em>, and <em>specificity </em>(each to three decimal places) <strong>(4 marks) </strong>Answer:

TP=96, TN=19, FP=8, FN=4 accuracy = 0.906 precision = 0.923 recall = 0.960 specificity = 0.704

<ol start="11">

 <li>Given the following models and AUC scores, find the corresponding ROC curve to each model. <strong>(1 mark)</strong></li>

</ol>

<ul>

 <li>Model 1 test set AUC score: 0.91 • Model 2 test set AUC score: 0.50</li>

 <li>Model 3 test set AUC score: 0.56</li>

</ul>

Answer:

<ul>

 <li>Model 1: ROC 1</li>

 <li>Model 2: ROC 3</li>

 <li>Model 3: ROC 2</li>

</ul>

<ol start="12">

 <li>A feature F1 can take certain value: A, B, C, D, E, &amp; F and represents grade of students from a college. Which of the following statement is true in following case? <strong>(1 mark)</strong>

  <ol>

   <li>Feature F1 is an example of nominal variable.</li>

   <li>Feature F1 is an example of ordinal variable.</li>

   <li>It doesn’t belong to any of the above categories.</li>

   <li>Both of a and b Answer:</li>

  </ol></li>

</ol>

b

<ol start="13">

 <li>[True or False] It is possible for a Pearson correlation between two variables to be zero but their values are still related to each other. <strong>(1 mark) </strong>Answer:</li>

</ol>

TRUE

<ol start="14">

 <li>Suppose you are given 7 scatter plots from 1 to 7 as below (from left to right) and you want to compare Pearson correlation coefficients between variables of each scatterplot.</li>

</ol>

Consider the following statements about the relative values of the coefficients.

<ol>

 <li>1&lt;2&lt;3&lt;4</li>

 <li>1&gt;2&gt;3&gt;4</li>

 <li>7&lt;6&lt;5&lt;4</li>

 <li>7&gt;6&gt;5&gt;4</li>

</ol>

Which pair of statements is correct? <strong>(1 mark)</strong>

<ol>

 <li>A and C</li>

 <li>B and C</li>

 <li>A and D</li>

 <li>B and D</li>

</ol>

Answer:

b

<ol start="15">

 <li>Run the code in the Notebook cell below and write <strong>one line </strong>of code for each of the following questions <strong>(5 marks)</strong></li>

</ol>

[1]: from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import precision_score, recall_score from sklearn.pipeline import Pipeline from sklearn.linear_model import SGDClassifier

X, y = make_classification(random_state=0)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) pipe = Pipeline([(‘scaler’, StandardScaler()), (‘classifier’,␣

<em>,</em><sub>→</sub>SGDClassifier(loss=’log’, random_state=42))]) pipe.fit(X_train, y_train);

<ol>

 <li>Compute the precision score of the model on the test set (1 mark):</li>

</ol>

[2]: precision_score(y_test, pipe.predict(X_test))

[2]: 0.7857142857142857

<ol>

 <li>Compute the recall score of the model on the test set (1 mark):</li>

</ol>

[3]: recall_score(y_test, pipe.predict(X_test))

[3]: 0.9166666666666666

<ol>

 <li>Predict the class of the last instance in the test set (1 mark):</li>

</ol>

[4]: array([1])

<ol>

 <li>Print the total number of instances in the test set which are correctly classified (2 marks):</li>

</ol>

[5]: 21